### **Computer**
- A computer is a general purpose devices that  can be programmed to process information, and yield meaningful results.
- Architecture :The view of a computer presented to software designers.
- Organisation : The actual implementation of a computer in hardware.
- computer type
  - desktop computer
    - other : watch , calculate,
---

### **Desktop computer**
- **main components (3):**
  - cpu / processor / computer
  - main memory / ram
  - hard disk / rom 
- **peripheral components** 
  - mouse , keyboard : for input
  - moniter , printer : for output

![alt](_resource/Pastedimage20241019124051.png)

---
### **Computer vs Our Brain**

![alt](_resource/Pasted%20image%2020241019124328.png)

---

### **Instruction**
- An instruction is defined as a basic command that can be given to a computer.
- computer can understand instruction like "show the time", they understand like - add a + b , multiply a , c
**instruction set**
  - The number of basic instructions/rudimentary commands that a processor can support have to be finite. This set of instructions is typically called the instruction set.
  - Some examples of basic instructions are: add, subtract, multiply, logical or, and logical not. 
  - each instruction needs to work on a set of variables and constants,and finally save the result in a variable.
  - driven by ISA (instruction set architecture)
 **ISA**
- The semantics of all the instructions supported by a processor is known as the instruction set architecture (ISA). 
- This includes the semantics of the instructions themselves, along with their operands, and interfaces with peripheral devices.
- ISA example:
	-  Intel and AMD CPUs use the x86 instruction set
	- IBM processors use the PowerPC® instruction set
	- HP processors use the PA-RISC instruction set
	- ARM processors use the ARM® instruction set ( variants Thumb-1 and Thumb-2)
- It is thus not possible to run a binary compiled for an Intel system on an ARM based system.
- example of intructions in an ISA
	- arthmetics instruction : add, sub, mul, div
	- logical instruction : and, or, not
	- data transfer/movement instruction

---

### **How to intruct a Computer**
![[Pasted image 20241116101010.png]]
- write program in programming language - c , c++ , java.
- compile it into a format that the computer understands (0s and 1s).
- execute the program.

---
### **Instruction Set Design**
- process of desiging an instruction for a processor
- We can think of an instruction set as a legal contract between software and hardware. The software part needs to ensure that all the programs that users write can be successfully and efficiently translated to basic instructions. Likewise, hardware needs to ensure that all the instructions in the instruction set are efficiently implementable.
- An ISA needs to have some necessary properties and some desirable properties for efficiency.
	- **Complete (necessary)** - The ISA should be able to Implement all User Programs.
	- **Concise** – Limited Size of the Instruction Set. We should ideally not have a lot of instructions .Implementing a lot of instructions will unnecessarily increase the number of transistors in the processor and increase its complexity.most instruction sets have somewhere between 64 to 1000 instructions.MIPS instruction set contains 64 instructions, whereas the Intel x86 instruction set has roughly a 1000 instructions as of 2012. Note that 1000 is considered a fairly large number for the number of instructions in an ISA.
	- **Generic** – Instructions should Capture the Common Case.Most of the common instructions in programs are simple arithmetic instructions such as add, subtract, multiply, divide. The most common logical instructions are logical and, or, exclusiveor, and not. Hence, it makes sense to dedicate an instruction to each of these common operations. It is not a good idea to have instructions that implement a very rarely used computation. For example, it might not make sense to implement an instruction that computes sin−1 (x).
	- **Simple** – Instructions should be Simple.

---
### **RISC and  CISC** 
**RISC**
- A reduced instruction set computer (RISC) implements simple instructions that have a simple and regular structure.
- The number of instructions is typically a small number (64 to 128). Examples: ARM, IBM PowerPC, HP PA-RISC.
**CISC**
- A complex instruction set computer (CISC) implements complex instructions that are highly irregular, take multiple operands, and implement complex functionalities.
- Secondly, the number of instructions is large (typically 500+). Examples: Intel x86, VAX
**Conclusion**
- Modern processors typically use a hybrid approach where they have simple, as well as some complicated instructions.
- However, under the hood CISC instructions are translated into RISC instructions. Hence, we believe that the scale tilts slightly more towards RISC instructions. We shall thus consider it a desirable property to have simple instructions.

---
### **Completeness of an ISA**
- ISA complete means , isa can implement all types of programs. 
- for example , if we just have add instructions , we can't subtract(so isa not complete).
- Ensure that we have just enough instruction such that we can implement every possible program that we might want to write.
- find answer of this question by universal machine.
---

### **Universal Machine**
- A machine that can execute any program is known as a universal machine. 
- We can treat each basic action of this machine as an instruction. Thus the set of actions of a universal machine is its ISA, and this ISA is complete. (universal ISA = universal machine = ISA complete).
- Alan Turing was the first to propose a universal machine that was extremely simple and powerful. This machine is aptly named after him, and is known as the Turing machine.

---
### **Turing Machine**
-  Alan Turing was the first to propose a universal machine that was extremely simple and powerful. This machine is aptly named after him, and is known as the Turing machine.
- A Turing Machine is a theoretical computational model introduced by **Alan Turing** in 1936. It is fundamental to the theory of computation and serves as a simplified abstract representation of a computer. Turing Machines are used to study what problems are computable and how efficiently they can be solved.
- Alan Turing know as father of the computer science.
![[Pasted image 20241116163830.png]]
**Components of a Turing Machine:**
1. **Infinite Tape**:  Represents the machine's memory. It is divided into discrete cells, each of which can hold a single symbol from a finite alphabet.
2. **Tape Head**: Can move left (L) or right (R) along the tape. It reads and writes symbols on the tape.
3. **State Register**:  Keeps track of the machine's current state.
4. **Transition Function / Action Table**: - Defines how the machine transitions between states based on:
    - The current state.
    - The symbol currently under the tape head.
- For each `(old state, old symbol)` pair, it specifies:
    - The new state.
    - The new symbol to write on the tape.
    - The direction to move the tape head (left or right).
**How turing machine work example** : 
 ![[Pasted image 20241116190213.png]]
- Turing machine is extremely simple, and extremely powerful.
- We can solve all kinds of problems – mathematical problems, engineering analyses, protein folding, computer games, …

---

### **Church-Turing thesis**
- Any real-world computation can be translated into an equivalent computation involving a Turing machine.(source: Wolfram Mathworld)
- Note: It is a thesis not theorem.
- For the last 60 years, nobody has found a counter-example
- Any computing system that is equivalent to a Turing machine is said to be Turing complete.

---
### **Universal Turing Machine**
- For every problem in the world we can design a turing machine (church-turing thesis).
- Can we design a universal turing machine that can simulate any turing machine. this will make it a universal machine (UTM).
- Why not? The logic of a Turing machine is really simple. We need to move the tape head left, or right, and update the symbol and state based on the action table. A UTM can easily do this.
- A UTM needs to have an action table, state register, and tape that can simulate any arbitrary Turing machine.

![[Pasted image 20241117084705.png]]
- It is possible to construct a universal Turing machine that can simulate any other Turing machine.
**UTM**
![[Pasted image 20241117084953.png]]
**Compare To Modren Computer**
![[Pasted image 20241117085839.png]]

**Comuter Inspired Form The Turing Machine**
![[Pasted image 20241117093538.png]]
- represents a universal machine that can be practically designed.
- **cpu**
	- program counter : points to an instruction in a program. After executing an instruction, it points to the next instruction by default. A branch instruction makes the PC point to another instruction (not in sequence).
	- control unit : perform control statement (loop).
	- arthmetic unit : perform arthmetic operations (eg: add , sub, multiplication).
- **memory** : contains  array of bytes 
	- instruction memory : sequence of instruction.(set of instruction called program) 
	- data memory  : variable and constants.
**Single Instruction ISA**
- it is possible to have a complete ISA that contains just a single instruction.
- **sbn (subtract and branch if negative)  a b c**
	- Everything is done using sbn ,even looping and condition.
	- Here, a, and b are memory locations. This instruction subtracts b from a, saves the results in a, and if a < 0, it jumps to the instruction at location c in the instruction table. Otherwise, the control transfers to the next instruction. For example, we can use this instruction to add two numbers saved in locations a and b. Note that exit is a special location at the end of the program.
	- example 1 : Adding Two Numbers
      ```assembly 
        #We can use this single instruction to perform addition. For example:
         1: sbn temp, b, 2  // temp = -b
         2: sbn a, temp, exit // a = a - (-b) => a = a + b
        ```
	
	- example 2 :  Adding Numbers from 1 to 10
   ```assembly
      Initial setup
      counter = 9 (tracks the number of iterations left).
      index = 10 (current number to add).
      one = 1 (used to decrement values).
      sum = 0 (stores the total sum).
     1: sbn temp, temp, 2  // temp = 0
     2: sbn temp, index, 3 // temp = -index
     3: sbn sum, temp, 4   // sum += index
     4: sbn index, one, 5  // index -= 1
     5: sbn counter, one, exit // counter -= 1; if negative, exit loop
     6: sbn temp, temp, 7  // temp = 0
     7: sbn temp, one, 1   // temp - 1 < 0; jump back to line 1
    ```

**Multiple Instructions ISA**
- Writing a program with just a single instruction is very difficult, and programs tend to be very long. There is no reason to be stingy with the number of instructions. We can make our life significantly easier by considering a multitude of instructions. Let us try to break up the basic sbn instructions into several instructions.
- **arthmetic instructions** : We can have a set of arithmetic instructions such as add, subtract, multiply and divide.
- **logical instructions** : We can have logical instruction such as or , and , not.
- **move instruction** : transfer values between memory locations. eg: mov , load , store
- **branch instrucions** : We require branch instructions that change the program counter to point to new instructions based on the results of computations or values stored in memory. eg : - Jump (`JMP`): Transfers control to another part of the program ,  Branch if Zero (`BEQZ`): Branches if a value is zero, Branch if Not Zero (`BNEZ`): Branches if a value is not zero, Call (`CALL`): Transfers control to a subroutine.

---

### **Design Of Practical Machine**
- After Alan Turing published his research paper proposing the Turing machine in 1936. Several scientists got inspired by his ideas, and started pursuing efforts to design practical machines.
- **Harvard Architecture**
	- One of the earliest efforts in this direction was the Harvard Mark-I.
	- Its block diagram is shown in Figure.
	- ![[Pasted image 20241117142723.png]]
	- There are separate structures for maintaining the instruction table and the memory. The former is also known as instruction memory because we can think of it as a specialised memory tailored to hold only instructions. The latter holds data values that programs need. Hence, it is known as the data memory.
	- The engine for processing instructions is divided into two parts – control and ALU. The job of the control unit is to fetch instructions, process them, and co-ordinate their execution. ALU stands for arithmetic-logic-unit. It has specialised circuits that can compute arithmetic expressions or logical expressions (AND/OR/NOT etc.).
	- Note that every computer needs to take inputs from the user/programmer and needs to finally communicate results back to the programmer. This can be done through a multitude of methods. Today we use a keyboard and monitor. Early computers used a set of switches and the final result was printed out on a piece of paper.
- **Von Neumann Architecture** 
	- John von Neumann proposed the Von Neumann architecture for general purpose Turing complete computers.
	- Note that there were several other scientists such as John Mauchly and J. Presper Eckert who independently developed similar ideas. Eckert and Mauchly designed the first general purpose Turing complete computer(with one minor limitation) called ENIAC (Electronic Numerical Integrator and Calculator) based on this architecture in 1946. It was used to compute artillery firing tables for the US army’s ballistic research laboratory. This computer was later succeeded by the EDVAC computer in 1949, which was also used by the US army’s ballistics research laboratory.
	- The basic Von Neumann architecture, which is the basis of ENIAC and EDVAC is shown in Figure. This is pretty much the same as our concept machine.
	- ![[Pasted image 20241117150504.png]]
	- The instruction table is saved in memory.(memory = instruction memory + data memory).
	- The processing engine that is akin to our modified Turing machine is called the CPU (central processing unit). It contains the program counter. Its job is to fetch new instructions, and execute them. It has dedicated functional units to calculate the results of arithmetic functions, load and store values in memory locations, and compute the results of branch instructions.
	- Lastly, like the Harvard architecture, the CPU is connected to the I/O subsystem.
	- The path breaking innovation in this machine was that the instruction table was stored in memory. It was possible to do so by encoding every instruction with the same set of symbols that are normally stored in memory. For example, if the memory stores decimal values, then each instruction needs to be encoded into a string of decimal digits.
	- A Von Neumann CPU needs to decode every instruction. The crux of this idea is that instructions are treated as regular data(memory values).This idea is known as the stored program concept.
	- **def**
		- Stored-program concept: A program is stored in memory and instructions are treated as regular memory values. 
- Lastly, astute readers would notice that a Von Neumann machine or a Harvard machine do not have an infinite amount of memory like a Turing machine. Hence, strictly speaking they are not exactly equivalent to a Turing machine. This is true for all practical machines. They need to have finite resources. Nevertheless, the scientific community has learnt to live with this approximation.

---
### **Problems with Harvard/ Von-Neumann Architectures**
- The memory is assumed to be one large array of bytes . It is very very slow.
- General Rule: Larger is a structure, slower it is.
- Solution : Have a small array of named locations (registers) that can be used by instructions ,This small array is very fast.
- \*Insight: Accesses exhibit locality (tend to use the same variables frequently in the same window of time).

### **Towards a Modern Machine with Registers , Stacks and Accumulator**
- Many extensions to the basic Von-Neumann machine have been proposed in literature. In fact this has been a hot field of study for the last half century. We discuss three important variants of Von Neumann machines that augment the basic model with registers, hardware stacks, and accumulators. The register based design is by far the most commonly used today.
- However, some aspects of stack based machines and accumulators have crept into modern register based processors also. It would be worthwhile to take a brief look at them before we move on.
- **1. Von-Neumann Machine with Registers**
	- A Von Neumann machine with registers enhances the traditional Von Neumann architecture by introducing registers, which are small, fast storage locations within the CPU. 
	- These registers significantly improve computational speed by reducing the need for frequent memory accesses during program execution.
	- Memory in modern systems can hold billions of locations but is relatively slow to access. The general rule: "Large is slow, small is fast.".
	- Registers are a small set of named locations (typically 8–64) that can be accessed very quickly. They hold temporary data used in computations, reducing dependence on slower memory.
	- Programs often reuse the same variables frequently (temporal locality). Keeping them in registers eliminates redundant memory accesses.
	- Data needed for computations is first fetched from memory and stored in registers using load instructions. Arithmetic and logical operations are performed using the data in registers.Results are written back to memory using  store instructions.
	- ![[Pasted image 20241117185513.png]]
	- Example : Adding the Cubes of Two Numbers
	  Suppose we want to calculate a=b3+c3a = b^3 + c^3a=b3+c3, where `b` and `c` are stored in memory. The computation involves:
      1. Loading `b` and `c` from memory into registers.
      2. Computing the cubes of `b` and `c` using registers.
      3. Adding the results and storing the final value in memory location a.
     ```assembly
       Here’s the sequence of instructions:

       1. r1 = mem[b]       // Load b into register r1 
       2. r2 = mem[c]       // Load c into register r2 
       3. r3 = r1 * r1      // Compute b² 
       4. r4 = r1 * r3      // Compute b³ 
       5. r5 = r2 * r2      // Compute c² 
       6. r6 = r2 * r5      // Compute c³ 
       7. r7 = r4 + r6      // Compute b³ + c³ 
       8. mem[a] = r7       // Store result in memory                                    location a
       Registers Used:
       r1 and r2 : Temporary registers for b and c.
       r3, r4, r5, and r6: Intermediate results.
       r7: Final result.
       ```
    
- 

- **2. Von-Neumann Machine with a Hardware Stack**
	- A stack is a standard data structure that obeys the semantics – last in, first out.
	-  A stack is built directly into the hardware of the computer. Data (like numbers) is placed into the stack from memory using a push operation. Arithmetic operations (e.g., addition, multiplication) work on the top values of the stack automatically.
	-  When you want to calculate something, you push the needed values onto the stack. Operations like addition or multiplication use the top values of the stack, replace them with the result, and automatically place the result back on top.
	- Example : To compute w=x+y / z - v . u
	     ```assembly
	        1.Push u onto the stack.
            2.Push v onto the stack.
            3.Multiply u and v, replace them with u⋅v.
            4.Push z and y.
            5. Divide y by z, replace them with y/z.
            6. Subtract u⋅v from y/z.
            7. Push x.
            8. Add x to the result of y/z−u⋅v.
            9.Pop the final result into w. 
             ```
    - Used in older systems like Burroughs Large Systems and HP 3000. Java programs are designed for a virtual stack machine (Java Virtual Machine), which makes them work on almost any device. 
    - In the machine Instructions are short because you don’t need to specify operands explicitly. Easier to build and check for errors. Useful for compact systems or devices where space is limited.
- **3. Von-Neumann Machine with Accumulator**
	- An accumulator is a single special-purpose register in the CPU used to store intermediate results of calculations. Think of it as a "scratchpad" for performing calculations step by step.
	- Every instruction involves the accumulator. For example, to add a number from memory, the CPU takes the value from memory, adds it to the value in the accumulator, and stores the result back in the accumulator. Similarly, for subtraction or multiplication, the accumulator holds the running result.
	- Example : To compute w = b<sup>3</sup> + c<sup>3</sup>
	      ```assembly
	      1.Load b into the accumulator.
          2.Multiply b by itself twice to compute b^3
          3.Save b^3 in memory or antoher register.
          4.Load c into the accumulator.
          5.Multiply c by itself twice to computer c^3
          6.Add b^3(from memory)to c^3 in the accumulator
          7.Store the final result in w
          ```
     -  Early computers in the 1950s often used accumulator-based designs because they couldn’t afford to include multiple registers.Some aspects of accumulator-based designs are still present in modern processors, like Intel x86, where specific registers (e.g., `eax`) act as accumulators for certain operations like multiplication.
     - Reduces the number of memory accesses, making calculations faster compared to directly working with memory. Simpler design compared to register-based systems.
     - imited flexibility due to reliance on a single accumulator. More memory operations may be required if intermediate results exceed the accumulator's capacity.

---

### **The Road Ahead**
- We have outlined the structure of a modern machine , which broadly follows a Von Neumann architecture, and is augmented with registers.
- It has a CPU with a program counter & registers, memory, and peripherals.
- The Instruction Set Architecture (ISA) is the link between hardware and software.
![[Pasted image 20241118094652.png]]
- ISA is Interface between software and hardware .A compiler converts a program into machine instructions in the given ISA.The processor executes the instructions in the ISA.
- Now, we need to proceed to build it. As mentioned at the outset, computer architecture is a beautiful amalgam of software and hardware. Software engineers tell us what to build? Hardware designers tell us how to build.
- ![[Pasted image 20241118101957.png]]
- We shall first look at the software aspect of the ISA (assembly programs) Then look at implementing the ISA by designing the processor Then, we shall make the computer more efficient by designing fast memory/ storage systems At the end, we will look at multiprocessors.

---

### **Representing Information**
- In modern computers, it is not possible to store numbers or pieces of text directly. Today’s computers are made of transistors.
- A transistor can be visualised as a basic switch that has two states – on and off. If the switch is on, then it represents 1, otherwise it represents 0. Every single entity inclusive of numbers, text, instructions, programs, and complex software needs to be represented using a sequence of 0s and 1s. We have only two basic symbols that we can use namely 0 and 1.
- A variable/value that can either be 0 or 1, is known as a bit. 
- Most computers typically store and process a set of 8 bits together. A set of 8 bits is known as a byte. 
- Typically, a sequence of 4 bytes is known as a word.
- We can thus visualise all the internal storage structures of a computer such as the memory or the set of registers as a large array of switches.
- ![[Pasted image 20241118101708.png]]